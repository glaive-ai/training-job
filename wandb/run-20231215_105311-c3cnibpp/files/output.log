
  0%|                                                                                                                                                                                                                                                                                                                                                                                                             | 0/3 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Traceback (most recent call last):
  File "/home/ubuntu/training-container/train.py", line 216, in train
    trainer.train()
  File "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1555, in train
    return inner_training_loop(
  File "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/trainer.py", line 1910, in _inner_training_loop
    self.optimizer.step()
  File "/home/ubuntu/.local/lib/python3.10/site-packages/accelerate/optimizer.py", line 142, in step
    self.optimizer.step(closure)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)